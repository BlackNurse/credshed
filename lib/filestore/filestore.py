#!/usr/bin/env python3

# by TheTechromancer

import os
import hashlib
import logging
from .util import *
import configparser
from ..errors import *
from pathlib import Path
from .decompress import *
from ..config import config


log = logging.getLogger('credshed.filestore')



class Filestore():

    meta_prefix = '.'
    meta_suffix = '.filestore'

    def __init__(self, store_dir=None):

        self.config = config['FILESTORE']
        if store_dir is None:
            self.dir = Path(self.config['store_dir']).resolve()
        else:
            self.dir = Path(store_dir).resolve()

        # cache to make sure a file doesn't get hashed twice
        self.hashes = {
            'md5': dict(),
            'sha1': dict(),
        }


    @property
    def files(self):
        '''
        Walks the filestore and yields each file as a Path() object
            - if a broken symlink is encountered, it is added to orphans
        '''

        if not self.dir.is_dir():
            log.error(f'Invalid filestore directory: {self.dir}')

        else:
            for filename in list_files(self.dir, include_symlinks=True):

                resolved_filename = filename.resolve()

                # skip files generated by this program
                if str(resolved_filename).endswith(self.meta_suffix) and \
                    str(resolved_filename).startswith(self.meta_prefix):
                    continue

                # skip the file if it links to something outside self.dir
                if not str(resolved_filename).startswith(str(self.dir)):
                    log.info(f'Skipping foreign symlink to {resolved_filename}')
                    continue

                if not filename.is_symlink():

                    # skip the file if it's been deleted or isn't a normal file
                    if not filename.is_file():
                        log.warning(f'{filename} does not exist or is not a normal file')
                        continue

                    # skip the file if it's empty or very small
                    else:
                        try:
                            # a@a.co == smallest valid email == 6 bytes
                            if size(filename) < 6:
                                log.info(f'Skipping empty / tiny file {resolved_filename}')
                                continue
                        except CredShedUtilError as e:
                            log.error(e)
                            continue

                yield filename



    def extract_files(self, delete=True):
        '''
        Scans the filestore for compressed files and attempts to extract them
        If extraction is successful, the archive is deleted
        '''

        for filename in self.files:
            if not filename.is_symlink():
                d = Decompress(filename)
                success = d.start()
                if success and delete:
                    self.delete_file(filename)


    def delete_file(self, filename):

        filename = Path(filename).resolve()
        self.write_metadata(filename)
        log.info(f'Deleting {filename}')
        filename.unlink()



    def write_metadata(self, filename):

        filename = Path(filename).resolve()
        meta_filename = self.meta_filename(filename)

        log.debug(f'Writing metadata for {filename} to {meta_filename}')

        sha1 = self.hash_file(filename, algo='sha1')
        md5 = self.hash_file(filename, algo='md5')

        try:
            with open(meta_filename, 'w') as f:
                f.write('[FILESTORE]\n')
                f.write(f'orig_filename={filename}\n')
                f.write(f'orig_sha1={sha1}\n')
                f.write(f'orig_md5={md5}\n')
        except OSError as e:
            raise FilestoreMetadataError(f'Failed to read metadata for {filename}: {e}')


    def read_metadata(self, filename):

        filename = Path(filename).resolve()

        log.debug(f'Reading metadata for {filename}')

        try:
            # read the file
            metadata = configparser.ConfigParser()
            metadata.read(self.meta_filename(filename))
            orig_filename = metadata['FILESTORE']['orig_filename']
            orig_sha1 = metadata['FILESTORE']['orig_sha1']
            orig_md5 = metadata['FILESTORE']['orig_md5']
            # update the hash cache
            self.hashes['md5'][filename] = orig_md5
            self.hashes['sha1'][filename] = orig_sha1

            return (orig_filename, orig_sha1)

        except (OSError, configparser.Error, KeyError) as e:
            raise FilestoreMetadataError(f'Failed to read metadata for {filename}: {e}')


    def hash_file(self, filename, algo='sha1', blocksize=65536):
        '''
        creates a SHA1 and MD5 hash in one go
        '''

        algo = algo.strip().lower()
        filename = Path(filename).resolve()

        try:
            filehash = self.hashes[algo][filename]
            log.debug(f'Hash for {filename} found in cache')
            return filehash

        except KeyError:

            log.info(f'Computing hash for {filename}')
            md5 = hashlib.md5()
            sha1 = hashlib.sha1()
            try:
                with open(str(filename), 'rb') as f:
                    block = f.read(blocksize)
                    while len(block) > 0:
                        sha1.update(block)
                        md5.update(block)
                        block = f.read(blocksize)

                md5_hash = md5.hexdigest()
                self.hashes['md5'][filename] = md5_hash
                sha1_hash = sha1.hexdigest()
                self.hashes['sha1'][filename] = sha1_hash
                return self.hashes[algo][filename]

            except OSError as e:
                raise FilestoreHashError(f'Failed to hash file {filename}: {e}')


    def meta_filename(self, filename):

        parent = Path(filename).parent
        new_filename = f'{self.meta_prefix}{Path(filename).name}{self.meta_suffix}'
        new_filename = parent / new_filename

        return new_filename