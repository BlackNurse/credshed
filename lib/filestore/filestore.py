#!/usr/bin/env python3

# by TheTechromancer

import os
import logging
from .util import *
import configparser
from ..errors import *
from pathlib import Path
from .decompress import *
from ..config import config
from ..processpool import ProcessPool


log = logging.getLogger('credshed.filestore')



class Filestore():

    meta_prefix = '.'
    meta_suffix = '.credshed'

    def __init__(self, store_dir=None):

        self.config = config['FILESTORE']
        if store_dir is None:
            self.dir = Path(self.config['store_dir']).resolve()
        else:
            self.dir = Path(store_dir).resolve()

        self._files = None

        # whether to delete archives after successful decompression
        self.delete = False


    @property
    def files(self):
        '''
        Walks the filestore and yields each file as a Path() object
            - if a broken symlink is encountered, it is added to orphans
        '''

        if self._files is None:

            self._files = []

            file_list = list(set(list_files(self.dir, include_symlinks=False)))

            for file in file_list:

                # skip files generated by this program
                if str(file).endswith(self.meta_suffix) and \
                    str(file).startswith(self.meta_prefix):
                    continue

                # skip the file if it links to something outside self.dir
                if not str(file).startswith(str(self.dir)):
                    log.debug(f'Skipping foreign symlink to {file}')
                    continue

                # skip the file if it's empty or very small
                try:
                    # a@a.co == smallest valid email == 6 bytes
                    if file.size < 6:
                        log.debug(f'Skipping empty / tiny file {file}')
                        continue
                except FileSizeError as e:
                    log.error(e)
                    continue

                self._files.append(file)

        return self._files


    def consolidate_files(self, clean=False, delete=True, size_threshold=2000000, count_threshold=50, dry_run=False):
        '''
        Scans for folders containing lots of files
        For each folder,
            Creates a single combined file for each extension and writes smaller files into it
        '''

        results = dict()

        log.info(f'Finding directories containing over {count_threshold:,} files in {self.dir}')
        for entry in os.listdir(self.dir):
            entry = Path(self.dir / entry)
            if entry.is_dir() and not entry.is_symlink():
                f = Filestore(entry)
                if len(f.files) >= count_threshold:
                    results[f.dir] = f._consolidate(
                        clean=clean,
                        delete=delete,
                        size_threshold=size_threshold,
                        count_threshold=count_threshold,
                        dry_run=dry_run
                    )

        log.info('Consolidation results:')
        results = list(results.items())
        results.sort(key=lambda x: len(x[-1]), reverse=True)
        for d, consolidated in results:

            log.info(d.relative_to(self.dir))
            log.info('=' * len(str(d.relative_to(self.dir))))
            consolidated = list(consolidated.items())
            consolidated.sort(key=lambda x: x[-1], reverse=True)
            for suffix, count in consolidated:
                log.info(f'{suffix:>15}{count:>15,}')


    def _consolidate(self, clean=False, delete=True, size_threshold=2000000, count_threshold=50, dry_run=False):
        '''
        Runs file consolidation on a single directory
        '''

        prefix = 'credshed.combined'

        log.info(f'Consolidating files in {self.dir}')

        to_consolidate = dict()
        consolidated = dict()

        # first, delete previous consolidations
        if clean:
            for file in self.files:
                if file.name.startswith(prefix):
                    file.unlink()

        for file in self.files:
            if file.size < size_threshold and not file.name.startswith(prefix):
                try:
                    to_consolidate[file.suffix.lower()].append(file)
                except KeyError:
                    to_consolidate[file.suffix.lower()] = [file]

        for suffix, files in to_consolidate.items():
            if len(files) >= count_threshold:
                log.debug(f'Consolidating {len(files):,} "{suffix}" files')
                for file in files:
                    if not dry_run:
                        combined_filename = self.dir / f'{prefix}{file.suffix}'
                        header = b'\n\n\{CREDSHED.COMBINED:' + \
                            str(file.relative_to(self.dir)).encode('utf-8', errors='ignore') + \
                            b'\}\n\n'
                        with open(combined_filename, 'ab') as w:
                            w.write(header)
                            with open(file, 'rb') as r:
                                w.write(r.read())
                                if delete:
                                    file.unlink()
                    try:
                        consolidated[file.suffix] += 1
                    except KeyError:
                        consolidated[file.suffix] = 1

        return consolidated


    def extract_files(self, delete=True, threads=2, force=False):
        '''
        Scans the filestore for compressed files and attempts to extract them
        If extraction is successful, the archive is deleted
        '''

        self.delete = delete

        Decompress.check_dependencies()

        files = [f for f in self.files if Decompress.is_compressed(f, force=force)]
        log.info(f'Found {len(files):,} potential archives')
        log.info(f'Extracting {len(files):,} using {threads:,} threads')

        with ProcessPool(threads) as pool:
            list(pool.map(self.extract_file, files))
                    


    def extract_file(self, file):

        if not file.is_symlink():
            d = Decompress(file)
            success = d.start()
            if success and self.delete:
                self.delete_file(file)


    def delete_file(self, file):

        self.write_metadata(file)
        log.info(f'Deleting {file}')
        file.unlink()



    def write_metadata(self, file):

        meta_filename = self.meta_filename(file)

        log.debug(f'Writing metadata for {file} to {meta_filename}')

        sha1 = file.hash(algo='sha1')
        md5 = file.hash(algo='md5')

        try:
            with open(meta_filename, 'w') as f:
                f.write('[CREDSHED]\n')
                f.write(f'orig_filename={file}\n')
                f.write(f'orig_sha1={sha1}\n')
                f.write(f'orig_md5={md5}\n')
        except OSError as e:
            raise FilestoreMetadataError(f'Failed to read metadata for {file}: {e}')


    def read_metadata(self, file):

        log.debug(f'Reading metadata for {file}')

        try:
            # read the file
            metadata = configparser.ConfigParser()
            metadata.read(self.meta_filename(file))
            orig_filename = metadata['CREDSHED']['orig_filename']
            orig_sha1 = metadata['CREDSHED']['orig_sha1']
            orig_md5 = metadata['CREDSHED']['orig_md5']

            return (orig_filename, orig_sha1)

        except (OSError, configparser.Error, KeyError) as e:
            raise FilestoreMetadataError(f'Failed to read metadata for {file}: {e}')


    def meta_filename(self, file):

        parent = Path(file).parent
        new_filename = f'{self.meta_prefix}{Path(file).name}{self.meta_suffix}'
        new_filename = parent / new_filename

        return new_filename